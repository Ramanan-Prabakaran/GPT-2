# GPT-2 Transformer Implementation in TensorFlow
This repository contains an implementation of a GPT-2 (Generative Pre-trained Transformer) model using TensorFlow and Keras. It includes key components like multi-head self-attention, feed-forward networks, and positional embeddings, mimicking the architecture of OpenAI's GPT-2.

